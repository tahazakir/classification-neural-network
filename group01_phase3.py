# -*- coding: utf-8 -*-
"""group01_Phase3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q2lN9rIc_AnlifS4prqj51nokWy5Pihi

# CS331 - Spring 2021 - Phase 3 [5%]

*__Submission Guidelines:__*
- Naming convention for submission of this notebook is `groupXX_Phase3.ipynb` where XX needs to be replaced by your group number. For example: group 1 would rename their notebook to `group01_Phase3.ipynb`
- Only the group lead is supposed to make the submission
- You need to submit this file as an .ipynb along with a .zip file of your own dataset and a document via LMS 
- The naming convention for the dataset is `groupXX_Dataset.zip`. Replace XX with the group number. The dataset <b>must</b> be a .zip file
- For the document, the naming convention is `groupXX_Document.pdf` where XX needs to be replaced by your group number. In the document, you have to tell why you chose the preprocessing methods that you chose and also justify the choice of network and the hyperparamters. Essentially, document the process of phase 3. The document should not be longer than 2 pages and less than 1 page
- All the cells <b>must</b> be run once before submission. If your submission's cells are not showing the results (plots etc.), marks wil be deducted
- Only the code written within this notebook will be considered while grading. No other files will be entertained
- You are advised to follow good programming practies including approriate variable naming and making use of logical comments 

The university honor code should be maintained. Any violation, if found, will result in disciplinary action.

#### <b>Introduction</b> 
This is the final phase of this offering's project. By now you guys have managed to build a multi-layer NN from scratch which is capable of scoring decent accuracy on the Fashion MNIST dataset. In this phase, we are going to shift our focus to the process of data engineering. You will be creating your own dataset (details will be disclosed as you follow along), you will have to carry out some preprocessing on this dataset of yours and will use this to evaluate your network from phase 2. This will also involove the tedious process of tweaking the hyperparamters so that your network scores higher accuracy scores on your own, novel, dataset

###### <b>IMPORTANT

###### Modification of the provided code without prior discussion with the TAs will result in a grade deduction</b>

---

###### <b>Side note</b>
The `plot_model` method will only work if you have the `pydot` python package installed along with [Graphviz](https://graphviz.gitlab.io/download/). If you do not wish to use this then simply comment out the import for `pydot`

###### <b>Need Help?</b>
If you need help, please refer to the course staff ASAP and do not wait till the last moment as they might not be available on very short notice close to deadlines

#### <b>PART 1</b>

This part is essentially the copy pasting of the codes from cells of your phase 2's notebook to the cells of this notebook. The intent is to replicate your phase 2's network and train (using the same OR different hyperparamters) it so that it can be used to evlaute the novel dataset (that will be part 2 of this phase)
"""

# Commented out IPython magic to ensure Python compatibility.
# making all the necessary imports here

import numpy as np
import pandas as pd
import time
# %matplotlib inline
import matplotlib.pyplot as plt
plt.style.use('seaborn')
from IPython.display import Image
import pydot
from tqdm import tqdm_notebook
import seaborn as sns
from keras.datasets import fashion_mnist
from sklearn.model_selection import train_test_split
from keras.utils import np_utils
from sklearn.datasets import make_moons
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.metrics import confusion_matrix,classification_report
from google.colab import drive
import glob
import cv2
from google.colab.patches import cv2_imshow
import skimage.exposure

# This function will be used to plot the confusion matrix at the end of this notebook

def plot_confusion_matrix(conf_mat):
    classes = ['T-shirt/top','Trouser/pants','Pullover shirt','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']
    df_cm = pd.DataFrame(conf_mat,classes,classes)
    plt.figure(figsize=(15,9))
    sns.set(font_scale=1.4)
    sns.heatmap(df_cm, annot=True,annot_kws={"size": 16})
    plt.show()

class_labels = ['T-shirt/top','Trouser/pants','Pullover shirt','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']

# Enter group lead's roll number here. This will be used for plotting purposes

rollnumber = 23100088

"""#### __Dataset from Keras Library__

The required library has been imported for you as fashion_mnist. Use it to load the train and test data accordingly.
"""

classes = 10 # Do not change this

# Download Fashion MNIST dataset
###### Code Here ######
data_set = fashion_mnist.load_data()

# Split the fashion MNIST dataset into train and test sets
# Convert y_train and y_test to categorical binary values 
###### Code Here ######
X_train_temp = data_set[0][0]
y_train_temp = data_set[0][1]
X_test_temp = data_set[1][0]
y_test_temp = data_set[1][1]
trainy = np_utils.to_categorical(y_train_temp)
testy = np_utils.to_categorical(y_test_temp)

#Reshape train and test images as one-dimensional arrays
###### Code Here ######
test_data = []
for i in range(X_test_temp.shape[0]):
  my_image = X_test_temp[i]
  normalized_image = (my_image - np.mean(my_image))/np.std(my_image)
  flatten_image = normalized_image.flatten()
  test_data.append(flatten_image)
testX = np.array(test_data)

train_data = []
for i in range(X_train_temp.shape[0]):
  my_image = X_train_temp[i]
  normalized_image = (my_image - np.mean(my_image))/np.std(my_image)
  flatten_image = normalized_image.flatten()
  train_data.append(flatten_image)
trainX = np.array(train_data)

print("Number of training sample: ", len(trainX))
print("Number of testing sample: ", len(testX))

"""#### __NN Implementation__
Your implementation of NN needs to use the `sigmoid` activation function for all hidden layers and the `softmax` activation function for the output layer. The NN model you will be creating here will consits of only three layers: 1 input layer, n hidden layers (where you have the liberty to define n) and 1 output layer.
"""

class LessHiddenLayers(Exception):
    def __init__(self, message):
      self.message = message

class NeuralNetwork():
    @staticmethod
    def cross_entropy_loss(y_pred, y_true):
        # implement cross_entropy_loss function
        #TO DO
        entropy_loss = -(y_true * np.log(y_pred)).sum()
        return entropy_loss

    @staticmethod
    def accuracy(y_pred, y_true):
        # implement accuracy function
        #TO DO
        accuracy = np.sum(y_pred == y_true)
        return (accuracy/y_pred.shape[0]) * 100

    @staticmethod
    def softmax(x):
        # implement softmax function
        #TO DO
        take_exponent = np.exp(x)
        soft_max = take_exponent / take_exponent.sum(axis=1, keepdims=True)
        return soft_max

    @staticmethod
    def sigmoid(x):
        #TO DO
        sigmoid_function = 1/(1 + np.exp(-x))
        return sigmoid_function

    def __init__(self, nodes_per_layer):
        '''Creates a Feed-Forward Neural Network.
        The parameters represent the number of nodes in each layer. 
        Look at the inputs to the function, and use 'try and accept'
        to catch errors if number of layers are < 2.
        '''
        error = False
        try:
          if len(nodes_per_layer) == 3:
            raise Exception
          self.num_layers = len(nodes_per_layer) # including input and output layers
          self.nodes_per_layer = nodes_per_layer
          self.input_shape = nodes_per_layer[0]
          self.output_shape = nodes_per_layer[-1]

          self.weights_ = []
          self.biases_ = []
          self.__init_weights(nodes_per_layer)
        except:
          error = True
        
        if error:
          raise LessHiddenLayers("Number of hidden layers should be more than 1.")

    def __init_weights(self, nodes_per_layer):
        '''Initializes all weights based on standard normal distribution and all biases to 0.'''
        '''Initialize weights for each layer except the input layer, since it does not have weights.'''
        
        ###### Code Here ######
        for i in range(1, len(nodes_per_layer) - 1):
          W_h = np.random.normal(size=(nodes_per_layer[i - 1], nodes_per_layer[i])) * np.sqrt(2/(nodes_per_layer[i - 1] + nodes_per_layer[i]))
          b_h = np.zeros(shape=(nodes_per_layer[i],))
          self.weights_.append(W_h)
          self.biases_.append(b_h)

        W_o = np.random.normal(size=(nodes_per_layer[len(nodes_per_layer) - 2], nodes_per_layer[-1])) * np.sqrt(2/(nodes_per_layer[len(nodes_per_layer) - 2] + nodes_per_layer[-1]))
        b_o = np.zeros(shape=(nodes_per_layer[-1],))
        self.weights_.append(W_o)
        self.biases_.append(b_o)
    
    def forward_pass(self, input_data):
        '''Executes the feed forward algorithm.
        "input_data" is the input to the network in row-major form
        Returns "activations", which is a list of all layer outputs (excluding input layer of course)'''
        
        ###### Code Here ######
        activations = []
        hidden_layer_activations = input_data
        for i in range(0, len(self.weights_) - 1):
          hidden_layer_activations = np.matmul(hidden_layer_activations, self.weights_[i]) + self.biases_[i]
          hidden_layer_activations = self.sigmoid(hidden_layer_activations)
          activations.append(hidden_layer_activations)

        output_layer_activations = np.matmul(hidden_layer_activations, self.weights_[-1]) + self.biases_[-1]
        output_layer_activations = self.softmax(output_layer_activations)
        activations.append(output_layer_activations)

        return activations

    def backward_pass(self, targets, layer_activations):
        '''Executes the backpropogation algorithm.
        "targets" is the ground truth/labels
        "layer_activations" are the return value of the forward pass step
        Returns "deltas", which is a list containing weight update values for all layers (excluding the input layer of course)'''
        
        ###### Code Here ######
        deltas = []
        dError_dOutput = layer_activations[-1] - targets
        dError_dInput = dError_dOutput
        deltas.append(dError_dInput)

        hidden_layer_number = len(layer_activations) - 2
        for i in range(len(self.weights_) - 1, 0, -1):
          dError_dOutput = np.matmul(dError_dInput, self.weights_[i].T)     
          dOutput_dInput = np.multiply(layer_activations[hidden_layer_number], (1 - layer_activations[hidden_layer_number]))
          hidden_layer_number -= 1
          dError_dInput = np.multiply(dOutput_dInput, dError_dOutput)
          deltas.append(dError_dInput)

        return deltas
    
    def weight_update(self, deltas, layer_inputs, lr):
        '''Executes the gradient descent algorithm.
        "deltas" is return value of the backward pass step
        "layer_inputs" is a list containing the inputs for all layers (including the input layer)
        "lr" is the learning rate'''
        
        ###### Code Here ######
        weight_number = 0
        layer_number = 0
        for i in range(len(deltas) - 1, 0, -1):
          dInput_dWeight = layer_inputs[layer_number]
          dError_dweight = np.matmul(dInput_dWeight.T, deltas[i])
          self.weights_[weight_number] = self.weights_[weight_number] - (lr * dError_dweight)
          self.biases_[weight_number] = self.biases_[weight_number] - (lr * np.sum(deltas[i], axis=0))
          weight_number += 1
          layer_number += 1
        
        dInput_dWeight = layer_inputs[-1]
        dError_dweight = np.matmul(dInput_dWeight.T, deltas[0])
        self.weights_[-1] = self.weights_[-1] - (lr * dError_dweight)
        self.biases_[-1] = self.biases_[-1] -  (lr * np.sum(deltas[0], axis=0))
  
    
    ###### Do Not Change Anything Below this line in This Cell ######
    
    def fit(self, Xs, Ys, epochs, lr=1e-3):
            history = []
            for epoch in tqdm_notebook(range(epochs)):
                num_samples = Xs.shape[0]
                for i in range(num_samples):

                    sample_input = Xs[i,:].reshape((1,self.input_shape))
                    sample_target = Ys[i,:].reshape((1,self.output_shape))
                    
                    activations = self.forward_pass(sample_input)   # Call forward_pass function 
                    deltas = self.backward_pass(sample_target, activations)    # Call backward_pass function 
                    layer_inputs = [sample_input] + activations[:-1]
                    
                    # Call weight_update function 
                    self.weight_update(deltas, layer_inputs, lr)
                
                preds = self.predict(Xs)   # Call predict function 

                current_loss = self.cross_entropy_loss(preds, Ys)
                
                if  epoch==epochs-1:
                  confusion_mat=confusion_matrix(Ys.argmax(axis=1), preds.argmax(axis=1),labels=np.arange(10))  
                  plot_confusion_matrix(confusion_mat)
                  report = classification_report(Ys, np_utils.to_categorical(preds.argmax(axis=1),num_classes=classes), target_names=class_labels)
                  print(report)
                history.append(current_loss)
            return history
    
    def predict(self, Xs):
        '''Returns the model predictions (output of the last layer) for the given "Xs".'''
        predictions = []
        num_samples = Xs.shape[0]
        for i in range(num_samples):
            sample = Xs[i,:].reshape((1,self.input_shape))
            sample_prediction = self.forward_pass(sample)[-1]
            predictions.append(sample_prediction.reshape((self.output_shape,)))
        return np.array(predictions)
    
    def evaluate(self, Xs, Ys):
        '''Returns appropriate metrics for the task, calculated on the dataset passed to this method.'''
        pred = self.predict(Xs)
        return self.cross_entropy_loss(pred, Ys), self.accuracy(pred.argmax(axis=1), Ys.argmax(axis=1))
    
    def plot_model(self, filename):
        '''Provide the "filename" as a string including file extension. Creates an image showing the model as a graph.'''
        graph = pydot.Dot(graph_type='digraph')
        graph.set_rankdir('LR')
        graph.set_node_defaults(shape='circle', fontsize=0)
        nodes_per_layer = [self.input_shape, self.hidden_shape, self.output_shape]
        for i in range(self.num_layers-1):
            for n1 in range(nodes_per_layer[i]):
                for n2 in range(nodes_per_layer[i+1]):
                    edge = pydot.Edge(f'l{i}n{n1}', f'l{i+1}n{n2}')
                    graph.add_edge(edge)
        graph.write_png(filename)

# These are what we call the hyperparameters (a.k.a Black Magic). You need to research on them and tweak them to see what generates the best result for you 

EPOCH = 10            # must be an int
LEARNING_RATE = 0.001
nodes_per_layer = [784, 16, 16, 10]  #int values for nodes of each layer. # of hidden layers >= 2.

start = time.time()

# Instantiate the neural network with the number of nodes you choose per layer, right now it is done for three layers only.
nn = NeuralNetwork(nodes_per_layer=nodes_per_layer)
history = nn.fit(trainX, trainy, epochs=EPOCH, lr=LEARNING_RATE)
plt.plot(history);
plt.gca().set(xlabel='Epoch', ylabel='Cross-entropy', title='Training Plot {}'.format(rollnumber));
end = time.time()

print("Runtime of the algorithm is ", round((end - start),3)," seconds")

# print accuracy on test set here
# Copy Code Here
print(nn.evaluate(testX, testy))

"""#### <b>PART 2</b>

Now that your NN model has been replicated and trained, we move on to the actual part of this phase i.e. making your own dataset. The instruction for creating your own dataset are as follows:

- Each group has to create a dataset of 50 images (preferred way to go about this is that each member takes 10 pictures for the dataset)
- Make sure that all the classes/categories that are there in the Fashion MNIST dataset are covered in these 50 pictures. 
- Make sure that that there is no class imbalance in your dataset. Class imbalance is when one or more classes have more images/data than others
- Once done with taking the pictures, add them into folders where the name of the folder will be the label of that image (for reference, you can have a look at how phase 1's dataset was organized as you essentially have to replicate that directory tree with the only difference is that there will only be `test` folder as this dataset is only being used for evaluation purposes)
- Zip the dataset and upload it on your Google Drive. You will have to mount your drive and import your dataset in the same way as it was done in phase 1

Once done, you will be reading the your dataset in the notebook and storing it in variables (please make sure that you are not overwriting any variable from Part 1). This will be followed by preprocessing that you will have to do on the images an finally, you will be able to evalute your model 

<b>Note: It is important that you know the classes/categories, shape and other information regarding the Fashion MNIST dataset on which your model is trained because for your own dataset, you will have to generate the data/images such that they are inline with those same properties and classes otherwise your network will make a lot of misclassifications. You can read up on the Fashion MNIST dataset [here](https://www.kaggle.com/zalando-research/fashionmnist): to familiarize yourself with the dataset</b>
"""

# Mounting Google Drive here
drive.mount('/drive')

# Edit this address so that it points to the dataset's zipped file on your Google Drive
!unzip -o -q "/drive/MyDrive/myDataSet.zip" -d "/content/data/"

classes = 10  # do not change this
my_X_test = None   # you must store the test images of your dataset in this varaible
my_y_test = None   # you must store the test images' labels of your dataset in this varaible
test_label = []

###### Code Here ######
'''Please note that you will have to extarct and one-hot encode the labels of the images for my_y_test'''
test_data = []
kernel = np.ones((5,5),np.uint8)
for image in glob.glob('/content/data/myDataSet/test/*/*'):
  test_label.append(image.split('/')[5])
  my_image = cv2.imread(image)
  my_image = cv2.resize(my_image,(256, 256))
  image_mask = cv2.inRange(my_image, lower, upper)
  image_mask = cv2.bitwise_not(image_mask, image_mask)
  my_image = cv2.bitwise_and(my_image, my_image, mask = image_mask)
  my_image = cv2.cvtColor(my_image, cv2.COLOR_BGR2GRAY)
  for i in range(my_image.shape[0]):
    for j in range(my_image.shape[1]):
      if my_image[i][j] != 0:
        my_image[i][j] = 255 - my_image[i][j]
  my_image = cv2.bilateralFilter(my_image,9,75,75)
  cv2_imshow(my_image)
  break
  test_data.append(my_image)

i = 0
my_y_test = np.zeros((50, 10))
for label in test_label:
  one_hot = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
  if label == "top":
    one_hot[0] = 1
    my_y_test[i] = one_hot
    i += 1
  elif label == "pants":
    one_hot[1] = 1
    my_y_test[i] = one_hot
    i += 1
  elif label == "shirt":
    one_hot[6] = 1
    my_y_test[i] = one_hot
    i += 1
  elif label == "sneaker":
    one_hot[7] = 1
    my_y_test[i] = one_hot
    i += 1
  elif label == "bag":
    one_hot[8] = 1
    my_y_test[i] = one_hot
    i += 1


#print("Number of images in the dataset: ", len(my_X_test))    # You can change len(X_test) based on your implementation such that total number of test samples is printed

"""Now you have to do the preprocessing of the images such that they are inline with the image properties of the images of Fashion MNIST dataset. This is the core task of this phase and completely dependent on your own research on preprocessing of data

<b>HINT: Data Normalization is one of the ways the data is preprocessed</b>
"""

'''Do data preprocessing here. Whatever you do here, please make sure that at the very end your images are in my_X_test and labels are in my_y_test'''
###### Code Here ######
test_data_temp = []
lower = np.array([120, 150, 150])
upper = np.array([255,255,255])
for image in test_data:
  my_image = image
  image_mask = cv2.inRange(my_image, lower, upper)
  image_mask = cv2.bitwise_not(image_mask, image_mask)
  my_image = cv2.bitwise_and(my_image, my_image, mask = image_mask)
  my_image = cv2.cvtColor(my_image, cv2.COLOR_BGR2GRAY)
  for i in range(my_image.shape[0]):
    for j in range(my_image.shape[1]):
      if my_image[i][j] != 0:
        my_image[i][j] = 255 - my_image[i][j]
  my_image = cv2.bilateralFilter(my_image,9,75,75)
  normalized_image = (my_image - np.mean(my_image))/np.std(my_image)
  flatten_image = normalized_image.flatten()
  test_data_temp.append(flatten_image)

my_X_test = np.array(test_data_temp)

print("Number of training sample: ", my_X_test.shape)
print("Number of testing sample: ", my_y_test.shape)

# print accuracy on your own dataset here
print(nn.evaluate(my_X_test,my_y_test))